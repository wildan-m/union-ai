{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Union.ai Support Engineer Interview\n",
    "\n",
    "## Wildan Muhlis - Technical Demonstration\n",
    "\n",
    "This notebook demonstrates my qualifications for the **Support Engineer** role at Union.ai by showcasing:\n",
    "\n",
    "1. **Technical expertise** in ML/data workflows and Flyte orchestration\n",
    "2. **Customer support mindset** through debugging and troubleshooting scenarios\n",
    "3. **Relevant experience** matching the job requirements\n",
    "4. **Problem-solving approach** for typical customer challenges\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Role Requirements Alignment\n",
    "\n",
    "### Job Requirements ‚Üí My Experience\n",
    "\n",
    "| **Requirement** | **My Background** | **Evidence** |\n",
    "|---|---|---|\n",
    "| **3+ years Data/Software Engineering** | ‚úÖ **12+ years** professional experience | Full-stack development, ML pipelines, enterprise systems across energy, finance, government sectors |\n",
    "| **Strong Python programming skills** | ‚úÖ **Advanced Python** with ML/data focus | 100+ projects, AI/ML implementations (OpenAI GPT-4, RAG), data processing automation |\n",
    "| **Public cloud technologies, containers** | ‚úÖ **AWS, Azure, Docker, K8s** | Production deployments (AWS EC2, S3), Azure DevOps pipelines, containerized applications |\n",
    "| **ML/data infrastructure experience** | ‚úÖ **Enterprise ML solutions** | Healthcare data analysis, automated training systems, analytics dashboards, AI-powered applications |\n",
    "| **Customer obsession & support experience** | ‚úÖ **Top-rated freelancer** (100% Job Success Score) | 40+ international clients, 12+ countries, maintained 100% satisfaction across complex technical projects |\n",
    "| **MLOps/DevOps in enterprise setting** | ‚úÖ **Enterprise automation & CI/CD** | Pertamina (Fortune 500 energy), government systems, Azure Pipelines, Jenkins, automated deployments |\n",
    "| **Excellent communication skills** | ‚úÖ **Proven international communication** | Technical documentation, stakeholder management, cross-cultural client relationships, training development |\n",
    "| **Familiarity with Spark, Airflow, TensorFlow** | ‚úÖ **Diverse tech stack mastery** | Rapid framework adaptation, distributed systems, modern AI/ML stack (transformers, torch, etc.) |\n",
    "\n",
    "### Key Differentiators\n",
    "- **Business acumen**: 12+ years translating technical solutions into business value for Fortune 500 companies\n",
    "- **International expertise**: Successfully worked across 6+ time zones with enterprise and government clients  \n",
    "- **Rapid learning**: Consistently mastered new technologies (React Native, AI/ML, cloud platforms) ahead of market trends\n",
    "- **Quality obsession**: Maintained 100% client satisfaction and top UpWork ratings throughout entire freelance career\n",
    "- **Enterprise scale**: Delivered systems handling 5000+ concurrent users, gigabyte data processing, mission-critical infrastructure\n",
    "- **Open source contributor**: Active Expensify contributor with 20+ merged PRs in React Native/TypeScript codebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Support Engineering Demonstration\n",
    "\n",
    "This section demonstrates my Support Engineer capabilities using the actual Flyte demo project in this repository:\n",
    "\n",
    "- **Project Setup**: Walk through the complete Flyte ML pipeline setup process\n",
    "- **Error Simulation**: Intentionally trigger common customer issues from the troubleshooting guide\n",
    "- **Issue Resolution**: Apply systematic debugging methodologies to resolve problems\n",
    "- **Customer Communication**: Provide clear, actionable guidance using established templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FLYTE DEMO PROJECT SETUP\n",
      "==================================================\n",
      "üìÅ Project Root: /Users/wildan/Repos/union-ai/flyte-interview-demo\n",
      "üìÇ Working Directory: /Users/wildan/Repos/union-ai/flyte-interview-demo\n",
      "\n",
      "üìã PROJECT STRUCTURE VERIFICATION:\n",
      "   ‚úÖ workflows/ml_pipeline.py\n",
      "   ‚úÖ workflows/debugging_scenarios.py\n",
      "   ‚úÖ data/sample_data.csv\n",
      "   ‚úÖ docs/troubleshooting-guide.md\n",
      "   ‚úÖ requirements.txt\n",
      "   ‚úÖ Dockerfile\n",
      "\n",
      "üêç PYTHON ENVIRONMENT:\n",
      "   Python Version: 3.12.11\n",
      "   Python Path: /Users/wildan/.local/bin/python3.12\n",
      "\n",
      "üì¶ DEPENDENCY CHECK:\n",
      "   ‚úÖ pandas - Available\n",
      "   ‚úÖ numpy - Available\n",
      "   ‚úÖ sklearn - Available\n",
      "   ‚úÖ matplotlib - Available\n",
      "   ‚úÖ seaborn - Available\n",
      "\n",
      "üöÄ Environment setup complete - Ready for Support Engineering demonstration!\n"
     ]
    }
   ],
   "source": [
    "# Project Setup and Environment Verification\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify we're in the correct project directory\n",
    "project_root = Path(\"/Users/wildan/Repos/union-ai/flyte-interview-demo\")\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"üîß FLYTE DEMO PROJECT SETUP\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Check project structure\n",
    "required_files = [\n",
    "    \"workflows/ml_pipeline.py\",\n",
    "    \"workflows/debugging_scenarios.py\", \n",
    "    \"data/sample_data.csv\",\n",
    "    \"docs/troubleshooting-guide.md\",\n",
    "    \"requirements.txt\",\n",
    "    \"Dockerfile\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìã PROJECT STRUCTURE VERIFICATION:\")\n",
    "for file_path in required_files:\n",
    "    full_path = project_root / file_path\n",
    "    status = \"‚úÖ\" if full_path.exists() else \"‚ùå\"\n",
    "    print(f\"   {status} {file_path}\")\n",
    "\n",
    "# Check Python environment and dependencies\n",
    "print(f\"\\nüêç PYTHON ENVIRONMENT:\")\n",
    "print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"   Python Path: {sys.executable}\")\n",
    "\n",
    "# Check required packages\n",
    "required_packages = ['pandas', 'numpy', 'sklearn', 'matplotlib', 'seaborn']\n",
    "print(f\"\\nüì¶ DEPENDENCY CHECK:\")\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"   ‚úÖ {package} - Available\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package} - Missing\")\n",
    "\n",
    "print(f\"\\nüöÄ Environment setup complete - Ready for Support Engineering demonstration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Scenario 1: Customer Workflow Setup and Execution\n",
    "\n",
    "**Customer Situation**: *\"New customer wants to run the ML pipeline demo but is encountering setup issues.\"*\n",
    "\n",
    "**Support Engineer Approach**: \n",
    "1. **Verify project setup** and dependencies\n",
    "2. **Run the baseline workflow** to ensure it works correctly\n",
    "3. **Document the successful execution** for customer reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ RUNNING BASELINE ML PIPELINE\n",
      "==================================================\n",
      "\n",
      "‚ùå IMPORT ERROR: No module named 'flytekit'\n",
      "üí° CUSTOMER GUIDANCE:\n",
      "   1. Verify all dependencies are installed: pip install -r requirements.txt\n",
      "   2. Check Python path configuration\n",
      "   3. Ensure workflow files are in correct location\n"
     ]
    }
   ],
   "source": [
    "def run_baseline_workflow():\n",
    "    \"\"\"\n",
    "    Execute the baseline ML pipeline to verify everything works correctly\n",
    "    This demonstrates the expected customer experience when setup is correct\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ RUNNING BASELINE ML PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Import the actual workflow from the project\n",
    "        sys.path.append('workflows')\n",
    "        from ml_pipeline import ml_training_pipeline\n",
    "        \n",
    "        print(\"üìä Step 1: Loading workflow components...\")\n",
    "        print(\"   ‚úÖ Successfully imported ml_training_pipeline\")\n",
    "        \n",
    "        print(\"\\nüöÄ Step 2: Executing complete ML training pipeline...\")\n",
    "        \n",
    "        # Execute the pipeline with default parameters\n",
    "        result = ml_training_pipeline(\n",
    "            data_path=\"data/sample_data.csv\",\n",
    "            test_size=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéâ PIPELINE EXECUTION SUCCESSFUL!\")\n",
    "        print(f\"   üìà Model Accuracy: {result.accuracy:.3f}\")\n",
    "        print(f\"   üìä Model Precision: {result.precision:.3f}\")\n",
    "        print(f\"   üìä Model Recall: {result.recall:.3f}\")\n",
    "        print(f\"   üìä Model F1-Score: {result.f1_score:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüíæ Generated Artifacts:\")\n",
    "        if os.path.exists(\"model.pkl\"):\n",
    "            print(\"   ‚úÖ model.pkl - Trained model saved successfully\")\n",
    "        else:\n",
    "            print(\"   ‚ùå model.pkl - Model file not found\")\n",
    "            \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'metrics': result,\n",
    "            'artifacts': ['model.pkl'] if os.path.exists(\"model.pkl\") else []\n",
    "        }\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n‚ùå DATA FILE ERROR: {str(e)}\")\n",
    "        print(\"üí° CUSTOMER GUIDANCE:\")\n",
    "        print(\"   1. Verify data file exists at: data/sample_data.csv\")\n",
    "        print(\"   2. Check file permissions\")\n",
    "        print(\"   3. Ensure correct working directory\")\n",
    "        return {'status': 'data_error', 'error': str(e)}\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"\\n‚ùå IMPORT ERROR: {str(e)}\")\n",
    "        print(\"üí° CUSTOMER GUIDANCE:\")\n",
    "        print(\"   1. Verify all dependencies are installed: pip install -r requirements.txt\")\n",
    "        print(\"   2. Check Python path configuration\")\n",
    "        print(\"   3. Ensure workflow files are in correct location\")\n",
    "        return {'status': 'import_error', 'error': str(e)}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå UNEXPECTED ERROR: {str(e)}\")\n",
    "        print(\"üí° ESCALATION NEEDED:\")\n",
    "        print(\"   1. Collect full error logs\")\n",
    "        print(\"   2. Check system resources\")\n",
    "        print(\"   3. Consult with Engineering team\")\n",
    "        return {'status': 'unknown_error', 'error': str(e)}\n",
    "\n",
    "# Execute baseline workflow\n",
    "baseline_result = run_baseline_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Scenario 2: Simulating Common Customer Issues\n",
    "\n",
    "**Customer Situation**: *\"Customer reports various workflow failures. We need to reproduce and resolve common issues from the troubleshooting guide.\"*\n",
    "\n",
    "**Support Engineer Approach**: \n",
    "1. **Simulate FileNotFoundError** - Most common data processing issue\n",
    "2. **Trigger Memory/Resource issues** - Resource constraint problems  \n",
    "3. **Create Import/Dependency errors** - Environment setup issues\n",
    "4. **Apply systematic troubleshooting** from our established guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SIMULATING: FileNotFoundError Issue\n",
      "==================================================\n",
      "‚ùå ERROR REPRODUCED: [Errno 2] No such file or directory: 'data/nonexistent_file.csv'\n",
      "\n",
      "üîß SUPPORT ENGINEER RESPONSE:\n",
      "üìß Customer Communication Template:\n",
      "\n",
      "Hi [Customer],\n",
      "\n",
      "I see you're experiencing a FileNotFoundError. This is one of the most common issues we help customers resolve.\n",
      "\n",
      "ERROR DIAGNOSIS:\n",
      "The workflow can't find the input data file at the specified path.\n",
      "\n",
      "SOLUTION STEPS:\n",
      "1. Verify the file path in your workflow configuration\n",
      "2. Check if the file exists at: data/nonexistent_file.csv\n",
      "3. Ensure the file has correct permissions\n",
      "4. Verify you're running from the correct working directory\n",
      "\n",
      "IMMEDIATE FIX:\n",
      "Try using the correct path: \"data/sample_data.csv\"\n",
      "\n",
      "Let me know if you need help locating your data files or configuring the correct paths.\n",
      "\n",
      "Best regards,\n",
      "Support Team\n",
      "        \n",
      "\n",
      "üí° RESOLUTION DEMONSTRATION:\n",
      "   ‚úÖ Successfully loaded data from: data/sample_data.csv\n",
      "   üìä Dataset shape: (10, 4)\n",
      "   üìã Columns: ['feature1', 'feature2', 'feature3', 'target']\n",
      "\n",
      "üîç SIMULATING: Memory Constraint Issue\n",
      "==================================================\n",
      "üé≠ CUSTOMER SCENARIO:\n",
      "   Customer reports: 'Tasks are being killed with OOMKilled error'\n",
      "   Current allocation: 1Gi memory\n",
      "   Dataset size: Large (>500MB)\n",
      "\n",
      "üìä MEMORY ANALYSIS:\n",
      "   Current dataset: 0.00 MB\n",
      "   Allocated memory: 1024 MB\n",
      "   Safety factor needed: 2-3x for processing\n",
      "   Recommended allocation: 0 MB\n",
      "\n",
      "üîß SUPPORT ENGINEER RESPONSE:\n",
      "üìß Customer Communication Template:\n",
      "\n",
      "Hi [Customer],\n",
      "\n",
      "I've analyzed your memory constraints. Your task needs more than the allocated 1Gi.\n",
      "\n",
      "DIAGNOSIS:\n",
      "- Dataset size: 0.0MB\n",
      "- Current allocation: 1Gi (1024MB)  \n",
      "- Processing overhead requires 2-3x dataset size\n",
      "\n",
      "IMMEDIATE SOLUTION:\n",
      "Update your task definition:\n",
      "\n",
      "```python\n",
      "from flytekit import Resources\n",
      "\n",
      "@task(\n",
      "    requests=Resources(mem=\"4Gi\"),\n",
      "    limits=Resources(mem=\"8Gi\")\n",
      ")\n",
      "def your_data_processing_task():\n",
      "    pass\n",
      "```\n",
      "\n",
      "ALTERNATIVE APPROACHES:\n",
      "1. Use chunked processing for large datasets\n",
      "2. Implement data streaming\n",
      "3. Consider data partitioning\n",
      "\n",
      "Would you like me to help implement chunked processing for your use case?\n",
      "\n",
      "Best regards,\n",
      "Support Team\n",
      "        \n",
      "\n",
      "üîç SIMULATING: Import/Dependency Error\n",
      "==================================================\n",
      "üé≠ CUSTOMER SCENARIO:\n",
      "   Customer reports: 'ModuleNotFoundError: No module named package'\n",
      "   Environment: Custom Docker container\n",
      "\n",
      "üì¶ DEPENDENCY CHECK:\n",
      "   ‚ùå flytekit - Missing\n",
      "   ‚ùå torch - Missing\n",
      "   ‚ùå transformers - Missing\n",
      "   ‚ùå custom_ml_lib - Missing\n",
      "\n",
      "üîß SUPPORT ENGINEER RESPONSE:\n",
      "üìß Customer Communication Template:\n",
      "\n",
      "Hi [Customer],\n",
      "\n",
      "I've identified missing dependencies in your environment.\n",
      "\n",
      "MISSING PACKAGES:\n",
      "- flytekit\n",
      "- torch\n",
      "- transformers\n",
      "- custom_ml_lib\n",
      "\n",
      "SOLUTION STEPS:\n",
      "\n",
      "1. Update your requirements.txt:\n",
      "```txt\n",
      "flytekit>=1.8.0\n",
      "torch>=2.0.0\n",
      "transformers>=4.30.0\n",
      "# Add other required packages\n",
      "```\n",
      "\n",
      "2. Rebuild your container image:\n",
      "```bash\n",
      "docker build -t your-image:v2.0 .\n",
      "```\n",
      "\n",
      "3. Update your task definitions:\n",
      "```python\n",
      "@task(container_image=\"your-registry/your-image:v2.0\")\n",
      "def ml_task():\n",
      "    import torch  # Now available\n",
      "    pass\n",
      "```\n",
      "\n",
      "4. For quick testing, install in current environment:\n",
      "```bash\n",
      "pip install flytekit torch  # Install available packages\n",
      "```\n",
      "\n",
      "Let me know if you need help with container configuration or dependency management.\n",
      "\n",
      "Best regards,\n",
      "Support Team\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "def simulate_data_file_error():\n",
    "    \"\"\"\n",
    "    Issue #1: FileNotFoundError - Most common customer issue\n",
    "    From troubleshooting guide section 2.1\n",
    "    \"\"\"\n",
    "    print(\"üîç SIMULATING: FileNotFoundError Issue\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Simulate customer's incorrect file path\n",
    "        wrong_path = \"data/nonexistent_file.csv\"\n",
    "        df = pd.read_csv(wrong_path)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå ERROR REPRODUCED: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüîß SUPPORT ENGINEER RESPONSE:\")\n",
    "        print(f\"üìß Customer Communication Template:\")\n",
    "        print(f'''\n",
    "Hi [Customer],\n",
    "\n",
    "I see you're experiencing a FileNotFoundError. This is one of the most common issues we help customers resolve.\n",
    "\n",
    "ERROR DIAGNOSIS:\n",
    "The workflow can't find the input data file at the specified path.\n",
    "\n",
    "SOLUTION STEPS:\n",
    "1. Verify the file path in your workflow configuration\n",
    "2. Check if the file exists at: {wrong_path}\n",
    "3. Ensure the file has correct permissions\n",
    "4. Verify you're running from the correct working directory\n",
    "\n",
    "IMMEDIATE FIX:\n",
    "Try using the correct path: \"data/sample_data.csv\"\n",
    "\n",
    "Let me know if you need help locating your data files or configuring the correct paths.\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "        ''')\n",
    "        \n",
    "        print(f\"\\nüí° RESOLUTION DEMONSTRATION:\")\n",
    "        correct_path = \"data/sample_data.csv\"\n",
    "        if os.path.exists(correct_path):\n",
    "            df = pd.read_csv(correct_path)\n",
    "            print(f\"   ‚úÖ Successfully loaded data from: {correct_path}\")\n",
    "            print(f\"   üìä Dataset shape: {df.shape}\")\n",
    "            print(f\"   üìã Columns: {list(df.columns)}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Correct file also missing - escalate to Engineering\")\n",
    "            \n",
    "def simulate_memory_constraint():\n",
    "    \"\"\"\n",
    "    Issue #2: Memory constraints - Resource allocation issue\n",
    "    From troubleshooting guide section 2.2\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç SIMULATING: Memory Constraint Issue\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"üé≠ CUSTOMER SCENARIO:\")\n",
    "    print(f\"   Customer reports: 'Tasks are being killed with OOMKilled error'\")\n",
    "    print(f\"   Current allocation: 1Gi memory\")\n",
    "    print(f\"   Dataset size: Large (>500MB)\")\n",
    "    \n",
    "    # Simulate memory analysis\n",
    "    try:\n",
    "        # Load sample data to analyze memory usage\n",
    "        df = pd.read_csv(\"data/sample_data.csv\")\n",
    "        memory_usage_mb = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "        \n",
    "        print(f\"\\nüìä MEMORY ANALYSIS:\")\n",
    "        print(f\"   Current dataset: {memory_usage_mb:.2f} MB\")\n",
    "        print(f\"   Allocated memory: 1024 MB\")\n",
    "        print(f\"   Safety factor needed: 2-3x for processing\")\n",
    "        print(f\"   Recommended allocation: {int(memory_usage_mb * 3)} MB\")\n",
    "        \n",
    "        print(f\"\\nüîß SUPPORT ENGINEER RESPONSE:\")\n",
    "        print(f\"üìß Customer Communication Template:\")\n",
    "        print(f'''\n",
    "Hi [Customer],\n",
    "\n",
    "I've analyzed your memory constraints. Your task needs more than the allocated 1Gi.\n",
    "\n",
    "DIAGNOSIS:\n",
    "- Dataset size: {memory_usage_mb:.1f}MB\n",
    "- Current allocation: 1Gi (1024MB)  \n",
    "- Processing overhead requires 2-3x dataset size\n",
    "\n",
    "IMMEDIATE SOLUTION:\n",
    "Update your task definition:\n",
    "\n",
    "```python\n",
    "from flytekit import Resources\n",
    "\n",
    "@task(\n",
    "    requests=Resources(mem=\"4Gi\"),\n",
    "    limits=Resources(mem=\"8Gi\")\n",
    ")\n",
    "def your_data_processing_task():\n",
    "    pass\n",
    "```\n",
    "\n",
    "ALTERNATIVE APPROACHES:\n",
    "1. Use chunked processing for large datasets\n",
    "2. Implement data streaming\n",
    "3. Consider data partitioning\n",
    "\n",
    "Would you like me to help implement chunked processing for your use case?\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "        ''')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in memory analysis: {str(e)}\")\n",
    "\n",
    "def simulate_import_dependency_error():\n",
    "    \"\"\"\n",
    "    Issue #3: Import/Dependency errors - Environment issue\n",
    "    From troubleshooting guide section 5.1\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç SIMULATING: Import/Dependency Error\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"üé≠ CUSTOMER SCENARIO:\")\n",
    "    print(f\"   Customer reports: 'ModuleNotFoundError: No module named package'\")\n",
    "    print(f\"   Environment: Custom Docker container\")\n",
    "    \n",
    "    # Simulate missing package check\n",
    "    missing_packages = []\n",
    "    test_imports = ['flytekit', 'torch', 'transformers', 'custom_ml_lib']\n",
    "    \n",
    "    print(f\"\\nüì¶ DEPENDENCY CHECK:\")\n",
    "    for package in test_imports:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"   ‚úÖ {package} - Available\")\n",
    "        except ImportError:\n",
    "            print(f\"   ‚ùå {package} - Missing\")\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(f\"\\nüîß SUPPORT ENGINEER RESPONSE:\")\n",
    "        print(f\"üìß Customer Communication Template:\")\n",
    "        print(f'''\n",
    "Hi [Customer],\n",
    "\n",
    "I've identified missing dependencies in your environment.\n",
    "\n",
    "MISSING PACKAGES:\n",
    "{chr(10).join([f\"- {pkg}\" for pkg in missing_packages])}\n",
    "\n",
    "SOLUTION STEPS:\n",
    "\n",
    "1. Update your requirements.txt:\n",
    "```txt\n",
    "flytekit>=1.8.0\n",
    "torch>=2.0.0\n",
    "transformers>=4.30.0\n",
    "# Add other required packages\n",
    "```\n",
    "\n",
    "2. Rebuild your container image:\n",
    "```bash\n",
    "docker build -t your-image:v2.0 .\n",
    "```\n",
    "\n",
    "3. Update your task definitions:\n",
    "```python\n",
    "@task(container_image=\"your-registry/your-image:v2.0\")\n",
    "def ml_task():\n",
    "    import torch  # Now available\n",
    "    pass\n",
    "```\n",
    "\n",
    "4. For quick testing, install in current environment:\n",
    "```bash\n",
    "pip install {' '.join(missing_packages[:2])}  # Install available packages\n",
    "```\n",
    "\n",
    "Let me know if you need help with container configuration or dependency management.\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "        ''')\n",
    "\n",
    "# Run error simulations\n",
    "simulate_data_file_error()\n",
    "simulate_memory_constraint()  \n",
    "simulate_import_dependency_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Scenario 3: Advanced Debugging with Project's Debug Scenarios\n",
    "\n",
    "**Customer Situation**: *\"Customer needs help with the debugging scenarios provided in our demo. They want to understand how to handle complex workflow failures.\"*\n",
    "\n",
    "**Support Engineer Approach**: \n",
    "1. **Execute debugging scenarios** from the project's debugging_scenarios.py\n",
    "2. **Demonstrate systematic troubleshooting** methodology\n",
    "3. **Provide comprehensive analysis** and resolution steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ADVANCED DEBUGGING DEMONSTRATION\n",
      "============================================================\n",
      "üìù INFO: Debugging scenarios module not available\n",
      "   This would typically be resolved by ensuring proper project setup\n",
      "\n",
      "üîß ALTERNATIVE DEBUGGING DEMONSTRATION:\n",
      "   Since the debugging module isn't available, here's how I would:\n",
      "   1. Investigate the import error\n",
      "   2. Check project structure and dependencies\n",
      "   3. Guide customer through proper setup\n",
      "   4. Provide working examples\n",
      "\n",
      "üìã TROUBLESHOOTING CHECKLIST:\n",
      "   1. Verify debugging_scenarios.py exists in workflows/\n",
      "   2. Check Python path configuration\n",
      "   3. Validate all required imports in the module\n",
      "   4. Test individual scenarios in isolation\n",
      "   5. Provide alternative debugging methods\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_debugging():\n",
    "    \"\"\"\n",
    "    Use the project's debugging_scenarios.py to show systematic troubleshooting\n",
    "    This demonstrates real-world Support Engineer workflow\n",
    "    \"\"\"\n",
    "    print(\"üîß ADVANCED DEBUGGING DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Import debugging scenarios from the actual project\n",
    "        from debugging_scenarios import (\n",
    "            simulate_memory_exhaustion,\n",
    "            simulate_timeout_issue,\n",
    "            simulate_data_validation_error,\n",
    "            simulate_dependency_conflict,\n",
    "            simulate_external_service_failure\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Successfully imported debugging scenarios from project\")\n",
    "        \n",
    "        # Scenario 1: Memory Exhaustion\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üß† DEBUGGING SCENARIO: Memory Exhaustion\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            result = simulate_memory_exhaustion()\n",
    "            print(\"üìä Scenario Result:\", result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Memory exhaustion triggered: {str(e)}\")\n",
    "            print(\"üîß SUPPORT ENGINEER ANALYSIS:\")\n",
    "            print(\"   Root Cause: Insufficient memory allocation for data processing\")\n",
    "            print(\"   Solution: Increase Resources(mem='4Gi') and implement chunking\")\n",
    "            print(\"   Prevention: Monitor memory usage during development\")\n",
    "        \n",
    "        # Scenario 2: Timeout Issues  \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"‚è±Ô∏è  DEBUGGING SCENARIO: Task Timeout\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            result = simulate_timeout_issue()\n",
    "            print(\"üìä Scenario Result:\", result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Timeout error triggered: {str(e)}\")\n",
    "            print(\"üîß SUPPORT ENGINEER ANALYSIS:\")\n",
    "            print(\"   Root Cause: Long-running operation exceeds default timeout\")\n",
    "            print(\"   Solution: Increase timeout and add progress monitoring\")\n",
    "            print(\"   Prevention: Profile execution time during development\")\n",
    "        \n",
    "        # Scenario 3: Data Validation\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä DEBUGGING SCENARIO: Data Validation Error\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            result = simulate_data_validation_error()\n",
    "            print(\"üìä Scenario Result:\", result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Data validation failed: {str(e)}\")\n",
    "            print(\"üîß SUPPORT ENGINEER ANALYSIS:\")\n",
    "            print(\"   Root Cause: Input data doesn't match expected schema\")\n",
    "            print(\"   Solution: Implement robust data validation and schema checking\")  \n",
    "            print(\"   Prevention: Use data contracts and comprehensive testing\")\n",
    "        \n",
    "        print(\"\\nüéØ DEBUGGING METHODOLOGY DEMONSTRATED:\")\n",
    "        print(\"   1. ‚úÖ Systematic error reproduction\")\n",
    "        print(\"   2. ‚úÖ Root cause analysis\")\n",
    "        print(\"   3. ‚úÖ Solution implementation\")\n",
    "        print(\"   4. ‚úÖ Prevention strategy development\")\n",
    "        print(\"   5. ‚úÖ Customer communication templates\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"üìù INFO: Debugging scenarios module not available\")\n",
    "        print(f\"   This would typically be resolved by ensuring proper project setup\")\n",
    "        \n",
    "        # Provide alternative demonstration\n",
    "        print(f\"\\nüîß ALTERNATIVE DEBUGGING DEMONSTRATION:\")\n",
    "        print(f\"   Since the debugging module isn't available, here's how I would:\")\n",
    "        print(f\"   1. Investigate the import error\")\n",
    "        print(f\"   2. Check project structure and dependencies\")\n",
    "        print(f\"   3. Guide customer through proper setup\")\n",
    "        print(f\"   4. Provide working examples\")\n",
    "        \n",
    "        # Show systematic troubleshooting approach\n",
    "        troubleshooting_steps = [\n",
    "            \"Verify debugging_scenarios.py exists in workflows/\",\n",
    "            \"Check Python path configuration\", \n",
    "            \"Validate all required imports in the module\",\n",
    "            \"Test individual scenarios in isolation\",\n",
    "            \"Provide alternative debugging methods\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüìã TROUBLESHOOTING CHECKLIST:\")\n",
    "        for i, step in enumerate(troubleshooting_steps, 1):\n",
    "            print(f\"   {i}. {step}\")\n",
    "\n",
    "# Execute advanced debugging demonstration\n",
    "demonstrate_advanced_debugging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Scenario 4: Model Explorer Tool Integration\n",
    "\n",
    "**Customer Situation**: *\"Customer successfully trained a model but needs help understanding its performance and characteristics for production deployment.\"*\n",
    "\n",
    "**Support Engineer Approach**: \n",
    "1. **Use the project's model explorer tool** to analyze the trained model\n",
    "2. **Provide comprehensive model analysis** for customer understanding\n",
    "3. **Guide customer through model interpretation** and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_model_analysis_support():\n",
    "    \"\"\"\n",
    "    Use the project's model_explorer.py tool to provide customer model analysis\n",
    "    This shows how Support Engineers can help customers understand their models\n",
    "    \"\"\"\n",
    "    print(\"üîç MODEL ANALYSIS SUPPORT DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if model exists from previous pipeline run\n",
    "    model_file = \"model.pkl\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"üìù INFO: Model file not found, generating one for demonstration...\")\n",
    "        \n",
    "        # Create a simple model for demonstration\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.datasets import make_classification\n",
    "        import joblib\n",
    "        \n",
    "        # Generate sample data and train model\n",
    "        X, y = make_classification(n_samples=1000, n_features=3, n_classes=2, random_state=42)\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Set feature names for the model (required for our explorer)\n",
    "        model.feature_names_in_ = ['feature1', 'feature2', 'feature3']\n",
    "        \n",
    "        # Save the model\n",
    "        joblib.dump(model, model_file)\n",
    "        print(f\"   ‚úÖ Created demonstration model: {model_file}\")\n",
    "    \n",
    "    # Now use the project's model explorer functionality\n",
    "    try:\n",
    "        print(f\"\\nüîß USING PROJECT'S MODEL EXPLORER TOOL:\")\n",
    "        print(f\"   Loading model_explorer.py functionality...\")\n",
    "        \n",
    "        # Import model explorer functions\n",
    "        import sys\n",
    "        sys.path.append('.')\n",
    "        from model_explorer import (\n",
    "            load_model,\n",
    "            basic_model_info,\n",
    "            model_parameters,\n",
    "            feature_importance_analysis\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úÖ Successfully imported model explorer functions\")\n",
    "        \n",
    "        # Load the model\n",
    "        print(f\"\\nüìã STEP 1: Loading and analyzing customer's model...\")\n",
    "        model = load_model(model_file)\n",
    "        \n",
    "        # Provide basic model information\n",
    "        print(f\"\\nüìä STEP 2: Basic Model Information\")\n",
    "        print(f\"   (This is what we'd share with the customer)\")\n",
    "        basic_model_info(model)\n",
    "        \n",
    "        # Show model parameters\n",
    "        print(f\"üìã STEP 3: Model Configuration Analysis\")\n",
    "        model_parameters(model)\n",
    "        \n",
    "        # Analyze feature importance (this creates visualizations)\n",
    "        print(f\"üéØ STEP 4: Feature Importance Analysis\")\n",
    "        print(f\"   Note: This creates feature_importance.png for customer\")\n",
    "        try:\n",
    "            feature_importance_analysis(model)\n",
    "        except Exception as e:\n",
    "            print(f\"   üìù Visualization skipped in notebook environment: {str(e)}\")\n",
    "            \n",
    "            # Provide text-based analysis instead\n",
    "            print(f\"   üìä Feature Importance (text format):\")\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = model.feature_names_in_\n",
    "            for name, importance in zip(feature_names, importances):\n",
    "                print(f\"     {name}: {importance:.4f} ({importance*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüí¨ CUSTOMER COMMUNICATION EXAMPLE:\")\n",
    "        print(f'''\n",
    "Hi [Customer],\n",
    "\n",
    "I've analyzed your trained model and here's what I found:\n",
    "\n",
    "MODEL SUMMARY:\n",
    "‚úÖ Model Type: Random Forest with {model.n_estimators} trees\n",
    "‚úÖ Features: {len(model.feature_names_in_)} input features\n",
    "‚úÖ Classes: {len(model.classes_)} target classes\n",
    "‚úÖ Performance: Ready for production deployment assessment\n",
    "\n",
    "KEY INSIGHTS:\n",
    "‚Ä¢ Your model uses all {len(model.feature_names_in_)} features effectively\n",
    "‚Ä¢ Feature importance analysis shows balanced feature usage\n",
    "‚Ä¢ Model complexity is appropriate for your dataset size\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Review the generated feature_importance.png visualization\n",
    "2. Consider A/B testing before full production deployment  \n",
    "3. Set up monitoring for model performance in production\n",
    "4. Plan for model retraining schedule\n",
    "\n",
    "I've also generated additional analysis files for your review. Let me know if you need help interpreting any of the results or planning your production deployment.\n",
    "\n",
    "Best regards,\n",
    "Support Team\n",
    "        ''')\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"üìù INFO: Model explorer functions not fully available\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        \n",
    "        # Provide alternative analysis\n",
    "        print(f\"\\nüîß ALTERNATIVE MODEL ANALYSIS:\")\n",
    "        print(f\"   Even without the full explorer, I can still help customers:\")\n",
    "        \n",
    "        import joblib\n",
    "        model = joblib.load(model_file)\n",
    "        \n",
    "        print(f\"\\nüìä BASIC MODEL ANALYSIS:\")\n",
    "        print(f\"   Model Type: {type(model).__name__}\")\n",
    "        print(f\"   Model Parameters: {len(model.get_params())} configuration options\")\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            print(f\"   Feature Importances Available: ‚úÖ\")\n",
    "            importances = model.feature_importances_\n",
    "            print(f\"   Top Feature: Importance {max(importances):.3f}\")\n",
    "        \n",
    "        print(f\"\\nüí° SUPPORT ENGINEER VALUE:\")\n",
    "        print(f\"   1. Can analyze models even with limited tools\")\n",
    "        print(f\"   2. Provide alternative analysis methods\")\n",
    "        print(f\"   3. Guide customers through model interpretation\")\n",
    "        print(f\"   4. Escalate to ML specialists when needed\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Execute model analysis demonstration\n",
    "model_analysis_success = demonstrate_model_analysis_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Scenario 5: Support KPI Management & Process Excellence\n",
    "\n",
    "**Customer Situation**: *\"Multiple customers experiencing various issues. We need to demonstrate systematic support process management and KPI tracking.\"*\n",
    "\n",
    "**Support Engineer Approach**: \n",
    "1. **Categorize and prioritize** customer issues systematically\n",
    "2. **Track resolution metrics** and identify improvement opportunities  \n",
    "3. **Document common patterns** for knowledge base enhancement\n",
    "4. **Demonstrate proactive support** through trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_support_kpi_management():\n",
    "    \"\"\"\n",
    "    Demonstrate systematic support operations and KPI management\n",
    "    This shows how I would approach support process excellence\n",
    "    \"\"\"\n",
    "    print(\"üìä SUPPORT KPI MANAGEMENT DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulate support ticket data based on common Flyte issues\n",
    "    support_tickets = [\n",
    "        {\n",
    "            'id': 'SUP-001', 'priority': 'High', 'category': 'Data Processing',\n",
    "            'issue': 'FileNotFoundError in workflow', 'status': 'Resolved',\n",
    "            'resolution_time_hours': 2, 'customer_type': 'Enterprise',\n",
    "            'resolution': 'Fixed file path configuration'\n",
    "        },\n",
    "        {\n",
    "            'id': 'SUP-002', 'priority': 'Critical', 'category': 'Resource Management', \n",
    "            'issue': 'OOMKilled - Memory exhaustion', 'status': 'Resolved',\n",
    "            'resolution_time_hours': 4, 'customer_type': 'SMB',\n",
    "            'resolution': 'Increased memory allocation to 4Gi'\n",
    "        },\n",
    "        {\n",
    "            'id': 'SUP-003', 'priority': 'Medium', 'category': 'Model Training',\n",
    "            'issue': 'Training timeout exceeded', 'status': 'Resolved', \n",
    "            'resolution_time_hours': 6, 'customer_type': 'Enterprise',\n",
    "            'resolution': 'Optimized training parameters and increased timeout'\n",
    "        },\n",
    "        {\n",
    "            'id': 'SUP-004', 'priority': 'High', 'category': 'Dependencies',\n",
    "            'issue': 'ModuleNotFoundError torch', 'status': 'Resolved',\n",
    "            'resolution_time_hours': 1, 'customer_type': 'SMB',\n",
    "            'resolution': 'Updated container image with missing dependencies'\n",
    "        },\n",
    "        {\n",
    "            'id': 'SUP-005', 'priority': 'Low', 'category': 'Documentation',\n",
    "            'issue': 'Workflow configuration questions', 'status': 'Resolved',\n",
    "            'resolution_time_hours': 0.5, 'customer_type': 'Enterprise', \n",
    "            'resolution': 'Provided configuration examples and documentation'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(support_tickets)\n",
    "    \n",
    "    print(\"üéØ SUPPORT TICKETS ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic KPI calculations\n",
    "    total_tickets = len(df)\n",
    "    resolved_tickets = len(df[df['status'] == 'Resolved'])\n",
    "    avg_resolution_time = df['resolution_time_hours'].mean()\n",
    "    \n",
    "    print(f\"üìä CORE KPIS:\")\n",
    "    print(f\"   Total Tickets: {total_tickets}\")\n",
    "    print(f\"   Resolved: {resolved_tickets} ({resolved_tickets/total_tickets*100:.1f}%)\")\n",
    "    print(f\"   Average Resolution Time: {avg_resolution_time:.1f} hours\")\n",
    "    print(f\"   SLA Compliance: {len(df[df['resolution_time_hours'] <= 4])/total_tickets*100:.1f}% (target: 90%)\")\n",
    "    \n",
    "    # Priority analysis\n",
    "    print(f\"\\nüìã PRIORITY BREAKDOWN:\")\n",
    "    priority_counts = df['priority'].value_counts()\n",
    "    for priority, count in priority_counts.items():\n",
    "        avg_time = df[df['priority'] == priority]['resolution_time_hours'].mean()\n",
    "        print(f\"   {priority}: {count} tickets (avg: {avg_time:.1f}h)\")\n",
    "    \n",
    "    # Category analysis \n",
    "    print(f\"\\nüîç ISSUE CATEGORY ANALYSIS:\")\n",
    "    category_counts = df['category'].value_counts()\n",
    "    for category, count in category_counts.items():\n",
    "        avg_time = df[df['category'] == category]['resolution_time_hours'].mean()\n",
    "        print(f\"   {category}: {count} tickets (avg: {avg_time:.1f}h)\")\n",
    "    \n",
    "    # Customer type analysis\n",
    "    print(f\"\\nüè¢ CUSTOMER TYPE BREAKDOWN:\")\n",
    "    customer_counts = df['customer_type'].value_counts()\n",
    "    for ctype, count in customer_counts.items():\n",
    "        avg_time = df[df['customer_type'] == ctype]['resolution_time_hours'].mean()\n",
    "        print(f\"   {ctype}: {count} tickets (avg: {avg_time:.1f}h)\")\n",
    "    \n",
    "    # Identify improvement opportunities\n",
    "    print(f\"\\nüí° IMPROVEMENT OPPORTUNITIES:\")\n",
    "    \n",
    "    # Find longest resolution times\n",
    "    slow_tickets = df[df['resolution_time_hours'] > avg_resolution_time]\n",
    "    if len(slow_tickets) > 0:\n",
    "        print(f\"   üî¥ {len(slow_tickets)} tickets exceeded average resolution time\")\n",
    "        slowest_category = slow_tickets['category'].mode().iloc[0]\n",
    "        print(f\"   üéØ Focus area: {slowest_category} issues need process improvement\")\n",
    "    \n",
    "    # Find common issues\n",
    "    common_patterns = df['category'].value_counts()\n",
    "    most_common = common_patterns.index[0]\n",
    "    print(f\"   üìö Knowledge Base: Create more documentation for {most_common} issues\")\n",
    "    \n",
    "    # Proactive recommendations\n",
    "    print(f\"\\nüöÄ PROACTIVE SUPPORT RECOMMENDATIONS:\")\n",
    "    recommendations = [\n",
    "        f\"Create automated checks for {most_common.lower()} issues\",\n",
    "        \"Develop customer self-service tools for common problems\",\n",
    "        \"Implement early warning system for resource constraints\", \n",
    "        \"Schedule preventive customer health checks\",\n",
    "        \"Build troubleshooting wizard for new customers\"\n",
    "    ]\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    # Generate support process documentation\n",
    "    print(f\"\\nüìã SUPPORT PROCESS EXCELLENCE:\")\n",
    "    print(f\"   ‚úÖ Systematic ticket categorization and prioritization\")\n",
    "    print(f\"   ‚úÖ Clear resolution tracking and SLA monitoring\")\n",
    "    print(f\"   ‚úÖ Pattern identification for process improvement\")\n",
    "    print(f\"   ‚úÖ Proactive recommendation development\")\n",
    "    print(f\"   ‚úÖ Customer communication template standardization\")\n",
    "    \n",
    "    # Mock escalation criteria\n",
    "    print(f\"\\nüö® ESCALATION CRITERIA:\")\n",
    "    escalation_rules = [\n",
    "        \"Critical issues (>4h): Escalate to Engineering\",\n",
    "        \"Customer complaints: Escalate to Customer Success\",\n",
    "        \"Platform bugs: Escalate with reproduction steps\",\n",
    "        \"Feature requests: Route to Product team\",\n",
    "        \"Security issues: Immediate Engineering escalation\"  \n",
    "    ]\n",
    "    \n",
    "    for rule in escalation_rules:\n",
    "        print(f\"   ‚Ä¢ {rule}\")\n",
    "    \n",
    "    return {\n",
    "        'total_tickets': total_tickets,\n",
    "        'resolution_rate': resolved_tickets/total_tickets,\n",
    "        'avg_resolution_time': avg_resolution_time,\n",
    "        'sla_compliance': len(df[df['resolution_time_hours'] <= 4])/total_tickets,\n",
    "        'top_category': most_common\n",
    "    }\n",
    "\n",
    "# Execute support KPI demonstration\n",
    "support_metrics = demonstrate_support_kpi_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Scenario 6: Performance Optimization & Resource Management\n",
    "\n",
    "**Customer Situation**: *\"Our workflows are expensive to run and we need to optimize costs while maintaining performance.\"*\n",
    "\n",
    "**My Approach**: Provide cost optimization strategies with monitoring recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_optimization_advisor():\n",
    "    \"\"\"\n",
    "    Cost optimization advisor for Union.ai customers\n",
    "    Demonstrates business value focus alongside technical expertise\n",
    "    \"\"\"\n",
    "    print(\"üí∞ COST OPTIMIZATION ADVISOR\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    optimization_strategies = {\n",
    "        'resource_right_sizing': {\n",
    "            'description': 'Optimize CPU and memory allocation',\n",
    "            'potential_savings': '30-50%',\n",
    "            'implementation': [\n",
    "                'Profile actual resource usage vs. allocated',\n",
    "                'Use resource requests vs. limits appropriately',\n",
    "                'Implement dynamic resource scaling',\n",
    "                'Use spot/preemptible instances where possible'\n",
    "            ],\n",
    "            'monitoring': [\n",
    "                'Track CPU and memory utilization metrics',\n",
    "                'Monitor resource waste percentage',\n",
    "                'Set up alerts for over/under-provisioning'\n",
    "            ]\n",
    "        },\n",
    "        'workflow_optimization': {\n",
    "            'description': 'Optimize workflow execution patterns',\n",
    "            'potential_savings': '20-40%',\n",
    "            'implementation': [\n",
    "                'Implement task-level caching',\n",
    "                'Use conditional execution (skip unnecessary tasks)',\n",
    "                'Parallelize independent tasks',\n",
    "                'Optimize data transfer between tasks'\n",
    "            ],\n",
    "            'monitoring': [\n",
    "                'Track workflow execution time',\n",
    "                'Monitor cache hit rates',\n",
    "                'Measure parallelization efficiency'\n",
    "            ]\n",
    "        },\n",
    "        'data_optimization': {\n",
    "            'description': 'Optimize data processing and storage',\n",
    "            'potential_savings': '25-35%',\n",
    "            'implementation': [\n",
    "                'Use efficient data formats (Parquet vs CSV)',\n",
    "                'Implement data compression',\n",
    "                'Minimize data movement between tasks',\n",
    "                'Use incremental processing'\n",
    "            ],\n",
    "            'monitoring': [\n",
    "                'Track data transfer volumes',\n",
    "                'Monitor storage costs',\n",
    "                'Measure data processing efficiency'\n",
    "            ]\n",
    "        },\n",
    "        'scheduling_optimization': {\n",
    "            'description': 'Optimize execution timing and scheduling',\n",
    "            'potential_savings': '15-25%',\n",
    "            'implementation': [\n",
    "                'Use off-peak hours for batch processing',\n",
    "                'Implement intelligent job scheduling',\n",
    "                'Batch similar workloads together',\n",
    "                'Use workflow prioritization'\n",
    "            ],\n",
    "            'monitoring': [\n",
    "                'Track peak vs off-peak usage',\n",
    "                'Monitor queue wait times',\n",
    "                'Measure cost per execution hour'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üéØ COST OPTIMIZATION STRATEGIES:\")\n",
    "    print()\n",
    "    \n",
    "    total_potential_savings = 0\n",
    "    \n",
    "    for strategy, details in optimization_strategies.items():\n",
    "        print(f\"üí° {strategy.upper().replace('_', ' ')}:\")\n",
    "        print(f\"   üìù {details['description']}\")\n",
    "        print(f\"   üí∞ Potential Savings: {details['potential_savings']}\")\n",
    "        \n",
    "        print(f\"   üîß Implementation Steps:\")\n",
    "        for step in details['implementation']:\n",
    "            print(f\"     - {step}\")\n",
    "        \n",
    "        print(f\"   üìä Monitoring & KPIs:\")\n",
    "        for metric in details['monitoring']:\n",
    "            print(f\"     - {metric}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Cost monitoring dashboard mock\n",
    "    print(\"üìä SAMPLE COST MONITORING DASHBOARD:\")\n",
    "    print()\n",
    "    \n",
    "    # Generate sample data for demonstration\n",
    "    sample_metrics = {\n",
    "        'current_month_cost': 1250.00,\n",
    "        'previous_month_cost': 1450.00,\n",
    "        'cost_per_workflow': 2.35,\n",
    "        'resource_utilization': 68,\n",
    "        'cache_hit_rate': 85,\n",
    "        'avg_execution_time': 12.5,\n",
    "        'workflows_executed': 532\n",
    "    }\n",
    "    \n",
    "    savings = sample_metrics['previous_month_cost'] - sample_metrics['current_month_cost']\n",
    "    savings_pct = (savings / sample_metrics['previous_month_cost']) * 100\n",
    "    \n",
    "    print(f\"   üí≥ Current Month Cost: ${sample_metrics['current_month_cost']:,.2f}\")\n",
    "    print(f\"   üìâ Month-over-Month Savings: ${savings:,.2f} ({savings_pct:.1f}%)\")\n",
    "    print(f\"   ‚ö° Cost per Workflow: ${sample_metrics['cost_per_workflow']:.2f}\")\n",
    "    print(f\"   üéØ Resource Utilization: {sample_metrics['resource_utilization']}%\")\n",
    "    print(f\"   üîÑ Cache Hit Rate: {sample_metrics['cache_hit_rate']}%\")\n",
    "    print(f\"   ‚è±Ô∏è Avg Execution Time: {sample_metrics['avg_execution_time']} minutes\")\n",
    "    print(f\"   üî¢ Workflows Executed: {sample_metrics['workflows_executed']:,}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üíº CUSTOMER SUCCESS IMPACT:\")\n",
    "    print(f\"   ‚úÖ Cost reduction achieved: {savings_pct:.1f}%\")\n",
    "    print(f\"   ‚úÖ Performance maintained/improved\")\n",
    "    print(f\"   ‚úÖ Resource efficiency optimized\")\n",
    "    print(f\"   ‚úÖ Monitoring and alerting implemented\")\n",
    "    \n",
    "    return sample_metrics\n",
    "\n",
    "# Run cost optimization analysis\n",
    "cost_metrics = cost_optimization_advisor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Summary: Excellence in Support Engineering at Union.ai\n",
    "\n",
    "This demonstration showcases my ability to fulfill Union.ai's core Support Engineer responsibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ WILDAN MUHLIS - UNION.AI SUPPORT ENGINEER\n",
      "=======================================================\n",
      "üèÜ SUPPORT ENGINEER RESPONSIBILITIES ALIGNMENT:\n",
      "\n",
      "üìã TECHNICAL ISSUE RESOLUTION:\n",
      "   üéØ Demonstrated Capabilities:\n",
      "     ‚úÖ Systematic debugging methodology honed across 12+ years and 100+ projects\n",
      "     ‚úÖ Enterprise-scale issue resolution (Pertamina, government systems, Expensify)\n",
      "     ‚úÖ Production debugging for 5000+ concurrent users and distributed systems\n",
      "     ‚úÖ Cross-platform troubleshooting (React Native, web, desktop, mobile)\n",
      "   üíº Business Impact: Proven track record resolving critical issues for Fortune 500 and government clients\n",
      "\n",
      "üìã CUSTOMER RETENTION & ADOPTION:\n",
      "   üéØ Demonstrated Capabilities:\n",
      "     ‚úÖ 100% Job Success Score on UpWork with 40+ international enterprise clients\n",
      "     ‚úÖ Technical education through YouTube channel and comprehensive documentation\n",
      "     ‚úÖ Proactive client relationship management across 6+ time zones\n",
      "     ‚úÖ Business value demonstration through ROI-focused project delivery\n",
      "   üíº Business Impact: 100% client retention rate, repeat business, and referral generation\n",
      "\n",
      "üìã CROSS-FUNCTIONAL PARTNERSHIP:\n",
      "   üéØ Demonstrated Capabilities:\n",
      "     ‚úÖ Open source contributor to Expensify (20+ merged PRs in React Native codebase)\n",
      "     ‚úÖ Cross-department collaboration at Pertamina (QHSSE, SCM, Management)\n",
      "     ‚úÖ International team coordination across development, DevOps, and business stakeholders\n",
      "     ‚úÖ Technical leadership roles managing teams of 3-4 developers\n",
      "   üíº Business Impact: Proven ability to work with Engineering, Product, and Business teams effectively\n",
      "\n",
      "üìã DOCUMENTATION & CUSTOMER EDUCATION:\n",
      "   üéØ Demonstrated Capabilities:\n",
      "     ‚úÖ Technical YouTube channel creator with educational content\n",
      "     ‚úÖ Comprehensive project documentation and video tutorials for client reusability\n",
      "     ‚úÖ Award-winning training system development (Gold CIP award at Pertamina)\n",
      "     ‚úÖ Multi-language technical communication (English, Indonesian, international clients)\n",
      "   üíº Business Impact: Self-sufficient customers, reduced support burden, enhanced knowledge transfer\n",
      "\n",
      "üìã AI/ML INFRASTRUCTURE EXPERTISE:\n",
      "   üéØ Demonstrated Capabilities:\n",
      "     ‚úÖ Modern AI/ML stack: OpenAI GPT-4, LangChain, RAG, speech-to-text/text-to-speech\n",
      "     ‚úÖ Production AI applications: MindLift quiz platform, Professional Profile Match\n",
      "     ‚úÖ AI-powered Discord bots with conversation context and session management\n",
      "     ‚úÖ Advanced data processing: Healthcare COVID-19 RDT data cleaning and analysis\n",
      "   üíº Business Impact: Deep understanding of modern ML workflows and customer challenges\n",
      "\n",
      "üöÄ WILDAN'S UNION.AI SUPPORT ENGINEER VALUE PROPOSITION:\n",
      "\n",
      "   üèÜ Exceptional Track Record: 12+ years, 100% client satisfaction, top-rated across platforms\n",
      "   üåê Global Enterprise Experience: Fortune 500 energy (Pertamina), government, international SMBs\n",
      "   ‚ö° Modern Tech Stack Mastery: AI/ML (GPT-4, LangChain), React Native, cloud platforms, DevOps\n",
      "   üéØ Customer Success Obsession: 100% Job Success Score demonstrates relentless customer focus\n",
      "   üîß Production-Scale Expertise: Systems handling 5000+ users, gigabyte data processing\n",
      "   ü§ù Cross-Cultural Communication: Successfully managed international clients across 12+ countries\n",
      "   üìö Technical Education Leader: YouTube creator, comprehensive documentation, training systems\n",
      "   üîÑ Rapid Technology Adoption: Consistently early adopter of emerging technologies and frameworks\n",
      "\n",
      "‚ö° IMMEDIATE IMPACT POTENTIAL:\n",
      "\n",
      "   üéØ Day 1: Leverage 12+ years debugging experience to resolve complex customer workflow issues\n",
      "   üìö Week 1: Apply proven documentation skills to enhance troubleshooting guides and FAQs\n",
      "   ü§ù Month 1: Use enterprise collaboration experience to partner with Engineering on improvements\n",
      "   üìà Quarter 1: Apply client success methodologies to measurably improve support KPIs\n",
      "   üåü Quarter 2: Lead technical education initiatives leveraging YouTube and training experience\n",
      "   üèÜ Quarter 3: Achieve exceptional retention metrics using proven 100% satisfaction methodologies\n",
      "\n",
      "üåü UNIQUE QUALIFICATIONS FOR UNION.AI:\n",
      "   üè≠ Enterprise Infrastructure Veterans: Pertamina (energy), government mission-critical systems\n",
      "   üöÄ Open Source Contributor: Active Expensify contributor, ready for Flyte ecosystem participation\n",
      "   üß† Modern AI/ML Practitioner: Production experience with GPT-4, LangChain, and emerging AI tools\n",
      "   üéñÔ∏è Award-Winning Solutions: Gold CIP awards for technical excellence and business impact\n",
      "   üåê International Scale: Proven success across cultures, time zones, and enterprise complexity levels\n",
      "\n",
      "üìû READY TO EXCEL AT UNION.AI:\n",
      "   üéØ Excited to apply 12+ years of technical excellence to Union.ai customer success\n",
      "   üí¨ Prepared to discuss specific enterprise support scenarios and complex technical challenges\n",
      "   ü§ù Looking forward to contributing to Union.ai's mission and the broader Flyte community\n"
     ]
    }
   ],
   "source": [
    "def final_interview_summary():\n",
    "    \"\"\"\n",
    "    Summary of demonstrated qualifications aligned with Union.ai's Support Engineer role\n",
    "    \"\"\"\n",
    "    print(\"üéØ WILDAN MUHLIS - UNION.AI SUPPORT ENGINEER\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    core_responsibilities = {\n",
    "        'Technical Issue Resolution': {\n",
    "            'demonstrated': [\n",
    "                'Systematic debugging methodology honed across 12+ years and 100+ projects',\n",
    "                'Enterprise-scale issue resolution (Pertamina, government systems, Expensify)',\n",
    "                'Production debugging for 5000+ concurrent users and distributed systems',\n",
    "                'Cross-platform troubleshooting (React Native, web, desktop, mobile)'\n",
    "            ],\n",
    "            'impact': 'Proven track record resolving critical issues for Fortune 500 and government clients'\n",
    "        },\n",
    "        'Customer Retention & Adoption': {\n",
    "            'demonstrated': [\n",
    "                '100% Job Success Score on UpWork with 40+ international enterprise clients',\n",
    "                'Technical education through YouTube channel and comprehensive documentation',\n",
    "                'Proactive client relationship management across 6+ time zones',\n",
    "                'Business value demonstration through ROI-focused project delivery'\n",
    "            ],\n",
    "            'impact': '100% client retention rate, repeat business, and referral generation'\n",
    "        },\n",
    "        'Cross-functional Partnership': {\n",
    "            'demonstrated': [\n",
    "                'Open source contributor to Expensify (20+ merged PRs in React Native codebase)',\n",
    "                'Cross-department collaboration at Pertamina (QHSSE, SCM, Management)',\n",
    "                'International team coordination across development, DevOps, and business stakeholders',\n",
    "                'Technical leadership roles managing teams of 3-4 developers'\n",
    "            ],\n",
    "            'impact': 'Proven ability to work with Engineering, Product, and Business teams effectively'\n",
    "        },\n",
    "        'Documentation & Customer Education': {\n",
    "            'demonstrated': [\n",
    "                'Technical YouTube channel creator with educational content',\n",
    "                'Comprehensive project documentation and video tutorials for client reusability',\n",
    "                'Award-winning training system development (Gold CIP award at Pertamina)',\n",
    "                'Multi-language technical communication (English, Indonesian, international clients)'\n",
    "            ],\n",
    "            'impact': 'Self-sufficient customers, reduced support burden, enhanced knowledge transfer'\n",
    "        },\n",
    "        'AI/ML Infrastructure Expertise': {\n",
    "            'demonstrated': [\n",
    "                'Modern AI/ML stack: OpenAI GPT-4, LangChain, RAG, speech-to-text/text-to-speech',\n",
    "                'Production AI applications: MindLift quiz platform, Professional Profile Match',\n",
    "                'AI-powered Discord bots with conversation context and session management',\n",
    "                'Advanced data processing: Healthcare COVID-19 RDT data cleaning and analysis'\n",
    "            ],\n",
    "            'impact': 'Deep understanding of modern ML workflows and customer challenges'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üèÜ SUPPORT ENGINEER RESPONSIBILITIES ALIGNMENT:\")\n",
    "    print()\n",
    "    \n",
    "    for responsibility, details in core_responsibilities.items():\n",
    "        print(f\"üìã {responsibility.upper()}:\")\n",
    "        print(f\"   üéØ Demonstrated Capabilities:\")\n",
    "        for demo in details['demonstrated']:\n",
    "            print(f\"     ‚úÖ {demo}\")\n",
    "        print(f\"   üíº Business Impact: {details['impact']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"üöÄ WILDAN'S UNION.AI SUPPORT ENGINEER VALUE PROPOSITION:\")\n",
    "    print()\n",
    "    \n",
    "    union_value = [\n",
    "        \"üèÜ Exceptional Track Record: 12+ years, 100% client satisfaction, top-rated across platforms\",\n",
    "        \"üåê Global Enterprise Experience: Fortune 500 energy (Pertamina), government, international SMBs\",\n",
    "        \"‚ö° Modern Tech Stack Mastery: AI/ML (GPT-4, LangChain), React Native, cloud platforms, DevOps\",\n",
    "        \"üéØ Customer Success Obsession: 100% Job Success Score demonstrates relentless customer focus\",\n",
    "        \"üîß Production-Scale Expertise: Systems handling 5000+ users, gigabyte data processing\",\n",
    "        \"ü§ù Cross-Cultural Communication: Successfully managed international clients across 12+ countries\",\n",
    "        \"üìö Technical Education Leader: YouTube creator, comprehensive documentation, training systems\",\n",
    "        \"üîÑ Rapid Technology Adoption: Consistently early adopter of emerging technologies and frameworks\"\n",
    "    ]\n",
    "    \n",
    "    for value in union_value:\n",
    "        print(f\"   {value}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"‚ö° IMMEDIATE IMPACT POTENTIAL:\")\n",
    "    print()\n",
    "    \n",
    "    immediate_impact = [\n",
    "        \"üéØ Day 1: Leverage 12+ years debugging experience to resolve complex customer workflow issues\",\n",
    "        \"üìö Week 1: Apply proven documentation skills to enhance troubleshooting guides and FAQs\", \n",
    "        \"ü§ù Month 1: Use enterprise collaboration experience to partner with Engineering on improvements\",\n",
    "        \"üìà Quarter 1: Apply client success methodologies to measurably improve support KPIs\",\n",
    "        \"üåü Quarter 2: Lead technical education initiatives leveraging YouTube and training experience\",\n",
    "        \"üèÜ Quarter 3: Achieve exceptional retention metrics using proven 100% satisfaction methodologies\"\n",
    "    ]\n",
    "    \n",
    "    for impact in immediate_impact:\n",
    "        print(f\"   {impact}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üåü UNIQUE QUALIFICATIONS FOR UNION.AI:\")\n",
    "    print(\"   üè≠ Enterprise Infrastructure Veterans: Pertamina (energy), government mission-critical systems\")\n",
    "    print(\"   üöÄ Open Source Contributor: Active Expensify contributor, ready for Flyte ecosystem participation\")\n",
    "    print(\"   üß† Modern AI/ML Practitioner: Production experience with GPT-4, LangChain, and emerging AI tools\")\n",
    "    print(\"   üéñÔ∏è Award-Winning Solutions: Gold CIP awards for technical excellence and business impact\")\n",
    "    print(\"   üåê International Scale: Proven success across cultures, time zones, and enterprise complexity levels\")\n",
    "    \n",
    "    print()\n",
    "    print(\"üìû READY TO EXCEL AT UNION.AI:\")\n",
    "    print(\"   üéØ Excited to apply 12+ years of technical excellence to Union.ai customer success\")\n",
    "    print(\"   üí¨ Prepared to discuss specific enterprise support scenarios and complex technical challenges\")\n",
    "    print(\"   ü§ù Looking forward to contributing to Union.ai's mission and the broader Flyte community\")\n",
    "\n",
    "# Display final summary showcasing Wildan's impressive qualifications\n",
    "final_interview_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¨ Contact Information\n",
    "\n",
    "**Wildan Muhlis**  \n",
    "üìß Email: wildevemail@gmail.com  \n",
    "üåê Website: https://wildanmuhlis.com  \n",
    "üíº LinkedIn: https://www.linkedin.com/in/wildan-muhlis  \n",
    "üîó GitHub: https://github.com/wildan-m  \n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates my readiness to excel as a Support Engineer at Union.ai, combining deep technical expertise with proven customer support experience. I'm excited to bring this skill set to help Union.ai's customers resolve complex technical challenges and achieve their ML infrastructure goals.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
